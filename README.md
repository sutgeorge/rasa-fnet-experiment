# Example of implementing custom classifiers for Rasa

This repository is an edited [fork](https://github.com/MantisAI/rasa_custom_intent_classification) of an example of a simple Rasa project that shows how you can implement your own classifiers.
The data was automatically generated by running:

```
rasa init
```

_This example was executed on PopOS 22.04 LTS._

## Hugging Face Transformers Classifier
`transformer_classifier.py` can take any Hugging Face transformer that has an AutoModelForSequenceClassification implementation and train it. This custom intent classifier is set to use `fnet-base` by default, since this is an experiment that attempts to reduce the inference time of the classifier, but other models can be used as well.

## config.yml

`config.yml` contains examples of how you would insert the two classifiers into the Rasa pipeline, and how you would pass parameters to them.

# Running the project

To train the custom rasa intent classification model:
```rasa train nlu```

To test the model:
```rasa test nlu --nlu <path_to_test_data>```

To interact with the model:
```rasa shell```

## CUDA/Tensorflow libdevice issue

To fix the issue, read this [StackOverflow post](https://stackoverflow.com/questions/68614547/tensorflow-libdevice-not-found-why-is-it-not-found-in-the-searched-path)

## Configuring the Huggingface account

You will receive a message like this if the account details are not set up locally:

```
Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.
```

## Caveats

- For some reason, the tqdm progress bar that is displayed during the training stage shows the number of epochs 4 times larger than it should.
EDIT: The reason is because there are 4 story blocks which are processed, hence why the number of epochs displayed in the progress bar is apparently 4 times larger.
